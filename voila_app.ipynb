{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "voila app.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dB5PXdlPECw"
      },
      "source": [
        "# NPI News Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lkz1nnBBw5NP"
      },
      "source": [
        "This prototype app collects 100 news articles from the last 24 hours using the [GDELT](https://www.gdeltproject.org/) API. Machine learning is used to score and sort the articles according to whether the story is about NPI changes or not. Relevance scorse range from 0 to 100. I manually labelled news headlines to create a training dataset for the ML algorithm which can be viewed [here](https://docs.google.com/spreadsheets/d/1pjC0M53ES8BP9jH52ngjQPmtkkd2onIsoALC6AQaL3U/edit?usp=sharing) for colleagues with access to google sheets.\n",
        "\n",
        "Please be patient as it is running on slow free infrastructure.\n",
        "\n",
        "![alt text](https://www.countryside-jobs.com/perch/resources/hero/newspapers-w1600h500.jpg \"Newspapers\")\n",
        "\n",
        "You can \n",
        "\n",
        "- change the source country of news articles such as inputting `spain` or `unitedkingdom` instead of `world` \n",
        "- try a different GDELT query string [[docs](https://blog.gdeltproject.org/gdelt-doc-2-0-api-debuts/v)]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBxRfY_gYhNP"
      },
      "source": [
        "# !pip install -Uqq fastbook\n",
        "# import fastbook\n",
        "# fastbook.setup_book()\n",
        "\n",
        "from ipywidgets import interact\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from IPython.display import HTML\n",
        "import urllib.request\n",
        "\n",
        "\n",
        "from fastai.text.all import *\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGFpx-qJYvVK"
      },
      "source": [
        "MODEL_URL = \"https://www.dropbox.com/s/wg7gaa9gz8lmfrm/classifier_2020-11-11_1807.pkl?dl=1\"\n",
        "_ = urllib.request.urlretrieve(MODEL_URL, \"classifier_2020-11-11_1807.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQkSMBXJYxCg"
      },
      "source": [
        "classifier = load_learner(\"classifier_2020-11-11_1807.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqpCv5lJeNV1"
      },
      "source": [
        "def get_articles(query: str, country: str):\n",
        "  url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
        "  if country == 'world':\n",
        "    query_modifier =  ''\n",
        "  else:\n",
        "    query_modifier = 'sourcecountry:{}'.format(country)\n",
        "  payload = {\n",
        "      'query': '{} {} sourcelang:english'.format(query, query_modifier),\n",
        "      'mode': \"ArtList\",\n",
        "      'format': 'RSS',\n",
        "      'maxrecords': 100,\n",
        "      'timespan':'1d'\n",
        "  }\n",
        "  r = requests.get(url, params=payload)\n",
        "  return(r)\n",
        "\n",
        "def parse_articles(r: requests.Response):\n",
        "  soup = BeautifulSoup(r.text, 'xml')\n",
        "  items = soup.find_all('item')\n",
        "  if len(items) > 0:\n",
        "    data = [{'headline':i.title.text, 'url':i.link.text} for i in items if i.link is not None]\n",
        "    df = pd.DataFrame(data)\n",
        "  else:\n",
        "    df = pd.DataFrame()\n",
        "  return(df)\n",
        "\n",
        "def output_prediction_table(query: str, country: str):\n",
        "  r = get_articles(query, country)\n",
        "  df = parse_articles(r)\n",
        "  if len(df) == 0:\n",
        "    return(None)\n",
        "  df = df.groupby('headline', as_index=False).agg(first)\n",
        "  headlines = df.headline.to_list()\n",
        "  with classifier.no_bar() as clf:\n",
        "    predictions = [clf.predict(headline)[2][1] for headline in headlines]\n",
        "  df['relevance'] = [float(p)*100 for p in predictions]\n",
        "  df['relevance'] = df['relevance'].round().astype(int)\n",
        "  # test_dl = classifier.dls.test_dl(headlines)\n",
        "  # df['score'] = [float(i[1]) for i in classifier.get_preds(dl=test_dl)[0]]\n",
        "  # df['score'] = np.round(df['score'] * 100).astype(int)\n",
        "  df = df.sort_values('relevance', ascending=False)\n",
        "\n",
        "  def make_href(row: pd.Series):\n",
        "    return '<a href=\"{}\">{}</a>'.format(row.url, row.headline)\n",
        "\n",
        "  df['headline'] = df.apply(make_href, axis = 1)\n",
        "  html_string = df[['relevance', 'headline']].to_html(index=False)\n",
        "  html_string = (\n",
        "      html_string.replace('&lt;', '<')\n",
        "      .replace('&gt;', '>')\n",
        "      .replace('&lt;/a&gt;', '/a')\n",
        "  )\n",
        "  return HTML(html_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz1zYn7Ynznu"
      },
      "source": [
        "interact(output_prediction_table, \n",
        "         query = \"(covid OR coronavirus OR virus OR pandemic) (rules OR restrictions OR shutdown OR measures OR lockdown)\",\n",
        "         country=\"world\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaq9BAzsDZsf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}